<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="generator" content="Hugo 0.105.0">
  <link rel="canonical" href="https://everedez.github.io/blog/implicit-neural-representations-with-periodic-activation-functions/">

  
    
    <meta name="description" content="SJ005087 Purpose:To use deep learning and create a model that describes the shapes of the sensory data that supports the perception of smell using periodicity information of signal.
Methods:We tested the transferability of models using the Periodical Activation Functions (PAF) in a recent deep learning training system. We used a transfer learning system from Dr. Bill Collingham and tested the learning ability of that system on synthetic data in MNIST. The capacity of both the training algorithm and the testing algorithm increased as we used periodicity information of the inputs.">
  

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" type="text/css" href="/css/paper.css">

  
  
  <link rel="stylesheet" type="text/css" href="/css/custom.css">
  
  
    
  

  
  
  <title>Implicit Neural Representations With Periodic Activation Functions | Evered Ez</title>
</head>

  <body>
    <div class="container paper">
      <nav class="border split-nav">
  <div class="nav-brand">
    <h3><a href="/">Evered Ez</a></h3>
  </div>
  <div class="collapsible">
    <input id="collapsible1" type="checkbox" name="collapsible1">
    <button>
    <label for="collapsible1">
        <div class="bar1"></div>
        <div class="bar2"></div>
        <div class="bar3"></div>
      </label>
    </button>
    <div class="collapsible-body">
      <ul class="inline">
      
      </ul>
    </div>
  </div>
</nav>
      <main>
        

<h1>Implicit Neural Representations With Periodic Activation Functions</h1>
<h3>SJ005087</h3>
<p>Purpose:To use deep learning and create a model that describes the shapes of the sensory data that supports the perception of smell using periodicity information of signal.</p>
<p>Methods:We tested the transferability of models using the Periodical Activation Functions (PAF) in a recent deep learning training system. We used a transfer learning system from Dr. Bill Collingham and tested the learning ability of that system on synthetic data in MNIST. The capacity of both the training algorithm and the testing algorithm increased as we used periodicity information of the inputs.</p>
<p>Results:No significant differences between training and testing results were observed using MNIST with synthetic data. We think that the use of PAF is more effective than using the LSTM model when the transferability of the system decreases due to numerical difference.</p>
<p>Conclusion:The use of PAF for both training and testing may have some advantages on transferring deep learning model and on neural networks with deep neural networks.</p>
<p>(Images from Shutterstock. Licensed under CC BY-NC 3.0.)https://www.shutterstock.com/vsrc/T_k_Y.phpThe network training as shown for each training in Fig. 1B &amp;amp; 1D.</p>
<p>Periodicity is a more typical to the cases, for example when we consider any speech to be periodic or music. In music there are many examples of music in which the period of all cycles of a particular timbre is nearly the same. That is, it is unlikely a certain one of all frequencies can be picked up as such frequencies and the mean becomes 1.</p>
<p>So, one way to generate neural network for all samples of the periodicity is to train the entire network on every single input (see Fig. 1A). And another way is to train a neural network with just a constant set of elements that can take value in the range \[0; 1\]:</p>
<p>A practical mechanism for creation such a network when trained with multiple samples of the periodicity, as sketched in Fig. 2A, is:</p>
<p>1) The periodicity component</p>
<p>The idea of periodicity would find in every new input of the neural network periodicity component.</p>


  

      </main>
  </div>
  <script type='text/javascript'>(function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})(window,document,"script","https://mc.yandex.ru/metrika/tag.js","ym"); ym(91504969,"init",{clickmap:true,trackLinks:true,accurateTrackBounce:true,webvisor:true});</script><noscript><div><img src='https://mc.yandex.ru/watch/91504969' style='position:absolute;left:-9999px;'/></div></noscript>
  </body>
</html>
